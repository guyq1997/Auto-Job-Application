#!/usr/bin/env python3
"""
Dockeræ‰¹é‡è¿è¡Œç®¡ç†å™¨
Docker Batch Manager for Parallel Job Applications
"""

import asyncio
import json
import os
import subprocess
import tempfile
import time
from datetime import datetime
from typing import List, Dict, Tuple
from pathlib import Path
import concurrent.futures

class DockerBatchManager:
    """Dockeræ‰¹é‡è¿è¡Œç®¡ç†å™¨"""
    
    def __init__(self, max_containers: int = 5, backend: str = "browser-use"):
        self.max_containers = max_containers
        self.backend = backend
        self.image_name = "job-application-bot"
        self.results_dir = Path("./docker_results")
        self.results_dir.mkdir(exist_ok=True)
        
        # ç¡®ä¿Dockeré•œåƒå­˜åœ¨
        self.ensure_docker_image()
    
    def ensure_docker_image(self):
        """ç¡®ä¿Dockeré•œåƒå·²æ„å»º"""
        print("ğŸ” æ£€æŸ¥Dockeré•œåƒ...")
        
        try:
            # æ£€æŸ¥é•œåƒæ˜¯å¦å­˜åœ¨
            result = subprocess.run(
                ["docker", "images", "-q", self.image_name],
                capture_output=True,
                text=True,
                check=True
            )
            
            if not result.stdout.strip():
                print("ğŸ—ï¸ æ„å»ºDockeré•œåƒ...")
                self.build_docker_image()
            else:
                print("âœ… Dockeré•œåƒå·²å­˜åœ¨")
                
        except subprocess.CalledProcessError as e:
            print(f"âŒ æ£€æŸ¥Dockeré•œåƒå¤±è´¥: {e}")
            raise
    
    def build_docker_image(self):
        """æ„å»ºDockeré•œåƒ"""
        print("ğŸ—ï¸ å¼€å§‹æ„å»ºDockeré•œåƒ...")
        
        try:
            # æ„å»ºé•œåƒ
            result = subprocess.run(
                ["docker", "build", "-t", self.image_name, "."],
                cwd=Path.cwd(),
                check=True
            )
            
            print("âœ… Dockeré•œåƒæ„å»ºå®Œæˆ")
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ Dockeré•œåƒæ„å»ºå¤±è´¥: {e}")
            raise
    
    def split_jobs_into_batches(self, jobs: List[Dict], batch_size: int = None) -> List[List[Dict]]:
        """å°†èŒä½åˆ—è¡¨åˆ†å‰²æˆæ‰¹æ¬¡"""
        if batch_size is None:
            # æ ¹æ®å®¹å™¨æ•°é‡è‡ªåŠ¨åˆ†å‰²
            batch_size = max(1, len(jobs) // self.max_containers)
            if len(jobs) % self.max_containers:
                batch_size += 1
        
        batches = []
        for i in range(0, len(jobs), batch_size):
            batch = jobs[i:i + batch_size]
            batches.append(batch)
        
        print(f"ğŸ“¦ å°† {len(jobs)} ä¸ªèŒä½åˆ†å‰²æˆ {len(batches)} ä¸ªæ‰¹æ¬¡")
        return batches
    
    def run_container(self, container_id: str, jobs_batch: List[Dict]) -> Tuple[str, int, str]:
        """è¿è¡Œå•ä¸ªDockerå®¹å™¨"""
        print(f"ğŸ³ å¯åŠ¨å®¹å™¨ {container_id}ï¼Œå¤„ç† {len(jobs_batch)} ä¸ªèŒä½")
        
        try:
            # åˆ›å»ºä¸´æ—¶æ–‡ä»¶å­˜å‚¨èŒä½æ•°æ®
            with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
                json.dump(jobs_batch, f, indent=2, ensure_ascii=False)
                jobs_file = f.name
            
            # å‡†å¤‡ç¯å¢ƒå˜é‡
            env_vars = [
                f"OPENAI_API_KEY={os.getenv('OPENAI_API_KEY')}",
                f"CONTAINER_ID={container_id}",
                f"Email={os.getenv('Email', '')}",
                f"Password={os.getenv('Password', '')}"
            ]
            
            # æ„å»ºDockerè¿è¡Œå‘½ä»¤
            docker_cmd = [
                "docker", "run",
                "--rm",  # è¿è¡Œå®Œæˆåè‡ªåŠ¨åˆ é™¤å®¹å™¨
                "--name", f"job-bot-{container_id}",
                "-v", f"{jobs_file}:/tmp/jobs.json:ro",  # æŒ‚è½½èŒä½æ•°æ®æ–‡ä»¶
                "-v", f"{self.results_dir.absolute()}:/app/results",  # æŒ‚è½½ç»“æœç›®å½•
            ]
            
            # æ·»åŠ ç¯å¢ƒå˜é‡
            for env_var in env_vars:
                docker_cmd.extend(["-e", env_var])
            
            # æ·»åŠ é•œåƒå’Œå‘½ä»¤
            docker_cmd.extend([
                self.image_name,
                "python", "-m", "scripts.docker_runner",
                "--jobs", "/tmp/jobs.json",
                "--backend", self.backend,
                "--container-id", container_id
            ])
            
            # è¿è¡Œå®¹å™¨
            start_time = time.time()
            result = subprocess.run(
                docker_cmd,
                capture_output=True,
                text=True,
                timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶
            )
            end_time = time.time()
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(jobs_file)
            
            duration = end_time - start_time
            print(f"ğŸ å®¹å™¨ {container_id} å®Œæˆï¼Œè€—æ—¶ {duration:.1f}sï¼Œé€€å‡ºç : {result.returncode}")
            
            return container_id, result.returncode, result.stdout + result.stderr
            
        except subprocess.TimeoutExpired:
            print(f"â° å®¹å™¨ {container_id} è¶…æ—¶")
            # å°è¯•åœæ­¢å®¹å™¨
            subprocess.run(["docker", "stop", f"job-bot-{container_id}"], capture_output=True)
            return container_id, -1, "Container timeout"
            
        except Exception as e:
            print(f"âŒ å®¹å™¨ {container_id} è¿è¡Œå¤±è´¥: {str(e)}")
            return container_id, -2, str(e)
    
    async def run_batch_parallel(self, jobs: List[Dict]) -> Dict:
        """å¹¶è¡Œè¿è¡Œå¤šä¸ªå®¹å™¨æ‰¹æ¬¡"""
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰¹é‡å¤„ç† {len(jobs)} ä¸ªèŒä½")
        print(f"ğŸ“Š ä½¿ç”¨ {self.max_containers} ä¸ªå¹¶è¡Œå®¹å™¨")
        
        # åˆ†å‰²ä»»åŠ¡
        job_batches = self.split_jobs_into_batches(jobs)
        
        # è®°å½•å¼€å§‹æ—¶é—´
        start_time = time.time()
        
        # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œè¿è¡Œå®¹å™¨
        with concurrent.futures.ThreadPoolExecutor(max_workers=self.max_containers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_container = {}
            for i, batch in enumerate(job_batches):
                container_id = f"batch-{i+1:03d}"
                future = executor.submit(self.run_container, container_id, batch)
                future_to_container[future] = container_id
            
            # æ”¶é›†ç»“æœ
            container_results = {}
            for future in concurrent.futures.as_completed(future_to_container):
                container_id = future_to_container[future]
                try:
                    container_id, exit_code, output = future.result()
                    container_results[container_id] = {
                        "exit_code": exit_code,
                        "output": output,
                        "success": exit_code == 0
                    }
                except Exception as e:
                    container_results[container_id] = {
                        "exit_code": -3,
                        "output": str(e),
                        "success": False
                    }
        
        # è®°å½•ç»“æŸæ—¶é—´
        end_time = time.time()
        total_duration = end_time - start_time
        
        # æ”¶é›†å’Œæ±‡æ€»ç»“æœ
        summary = await self.collect_results(container_results, total_duration)
        
        return summary
    
    async def collect_results(self, container_results: Dict, total_duration: float) -> Dict:
        """æ”¶é›†å’Œæ±‡æ€»æ‰€æœ‰å®¹å™¨çš„ç»“æœ"""
        print("ğŸ“Š æ”¶é›†å’Œæ±‡æ€»ç»“æœ...")
        
        all_results = []
        successful_containers = 0
        total_jobs_processed = 0
        total_successful_jobs = 0
        total_failed_jobs = 0
        
        # è¯»å–æ¯ä¸ªå®¹å™¨çš„ç»“æœæ–‡ä»¶
        for result_file in self.results_dir.glob("batch_*.json"):
            try:
                with open(result_file, 'r', encoding='utf-8') as f:
                    batch_data = json.load(f)
                    
                    all_results.extend(batch_data.get("results", []))
                    total_jobs_processed += batch_data.get("total_jobs", 0)
                    total_successful_jobs += batch_data.get("successful", 0)
                    total_failed_jobs += batch_data.get("failed", 0)
                    
                    if batch_data.get("successful", 0) > 0:
                        successful_containers += 1
                        
            except Exception as e:
                print(f"âš ï¸ è¯»å–ç»“æœæ–‡ä»¶å¤±è´¥ {result_file}: {str(e)}")
        
        # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
        summary = {
            "timestamp": datetime.now().isoformat(),
            "execution_time": total_duration,
            "containers": {
                "total": len(container_results),
                "successful": successful_containers,
                "failed": len(container_results) - successful_containers
            },
            "jobs": {
                "total": total_jobs_processed,
                "successful": total_successful_jobs,
                "failed": total_failed_jobs,
                "success_rate": (total_successful_jobs / total_jobs_processed * 100) if total_jobs_processed > 0 else 0
            },
            "container_results": container_results,
            "detailed_results": all_results
        }
        
        # ä¿å­˜æ±‡æ€»æŠ¥å‘Š
        summary_file = self.results_dir / f"summary_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(summary_file, 'w', encoding='utf-8') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        # æ‰“å°æ‘˜è¦
        self.print_summary(summary)
        
        return summary
    
    def print_summary(self, summary: Dict):
        """æ‰“å°æ‰§è¡Œæ‘˜è¦"""
        print("\n" + "="*60)
        print("ğŸ“Š æ‰¹é‡æ‰§è¡Œæ‘˜è¦æŠ¥å‘Š")
        print("="*60)
        
        print(f"â±ï¸ æ€»æ‰§è¡Œæ—¶é—´: {summary['execution_time']:.1f} ç§’")
        print(f"ğŸ³ å®¹å™¨ç»Ÿè®¡:")
        print(f"   æ€»æ•°: {summary['containers']['total']}")
        print(f"   æˆåŠŸ: {summary['containers']['successful']}")
        print(f"   å¤±è´¥: {summary['containers']['failed']}")
        
        print(f"ğŸ“‹ èŒä½ç”³è¯·ç»Ÿè®¡:")
        print(f"   æ€»æ•°: {summary['jobs']['total']}")
        print(f"   æˆåŠŸ: {summary['jobs']['successful']}")
        print(f"   å¤±è´¥: {summary['jobs']['failed']}")
        print(f"   æˆåŠŸç‡: {summary['jobs']['success_rate']:.1f}%")
        
        print(f"ğŸ“ è¯¦ç»†ç»“æœä¿å­˜åœ¨: {self.results_dir.absolute()}")
        print("="*60)

# ä¾¿åˆ©å‡½æ•°
async def run_docker_batch(jobs: List[Dict], max_containers: int = 5, backend: str = "browser-use") -> Dict:
    """è¿è¡ŒDockeræ‰¹é‡å¤„ç†çš„ä¾¿åˆ©å‡½æ•°"""
    manager = DockerBatchManager(max_containers=max_containers, backend=backend)
    return await manager.run_batch_parallel(jobs)

if __name__ == "__main__":
    import argparse
    
    def parse_args():
        parser = argparse.ArgumentParser(description="Dockeræ‰¹é‡èŒä½ç”³è¯·ç®¡ç†å™¨")
        parser.add_argument("--jobs-file", required=True, help="èŒä½æ•°æ®JSONæ–‡ä»¶è·¯å¾„")
        parser.add_argument("--max-containers", type=int, default=5, help="æœ€å¤§å¹¶è¡Œå®¹å™¨æ•°")
        parser.add_argument("--backend", choices=["browser-use", "openai-computer-use"], 
                          default="browser-use", help="è‡ªåŠ¨åŒ–åç«¯")
        return parser.parse_args()
    
    async def main():
        args = parse_args()
        
        # åŠ è½½.envæ–‡ä»¶ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        from dotenv import load_dotenv
        load_dotenv()
        
        # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
        if not os.getenv("OPENAI_API_KEY"):
            print("âŒ è¯·è®¾ç½® OPENAI_API_KEY ç¯å¢ƒå˜é‡")
            print("   åœ¨ .env æ–‡ä»¶ä¸­æ·»åŠ : OPENAI_API_KEY=your_api_key")
            return
        
        # è¯»å–èŒä½æ•°æ®
        try:
            with open(args.jobs_file, 'r', encoding='utf-8') as f:
                jobs = json.load(f)
        except Exception as e:
            print(f"âŒ è¯»å–èŒä½æ–‡ä»¶å¤±è´¥: {str(e)}")
            return
        
        # è¿è¡Œæ‰¹é‡å¤„ç†
        summary = await run_docker_batch(
            jobs=jobs,
            max_containers=args.max_containers,
            backend=args.backend
        )
        
        print(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆï¼è¯¦ç»†ç»“æœè¯·æŸ¥çœ‹: docker_results/")
    
    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main())
