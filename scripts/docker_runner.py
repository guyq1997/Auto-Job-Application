#!/usr/bin/env python3
"""
Dockerå®¹å™¨è¿è¡Œè„šæœ¬
Docker Container Runner Script
"""

import asyncio
import json
import os
import sys
import argparse
from typing import Dict, List
from datetime import datetime

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append('/app')

try:
    from scripts.unified_automation import create_unified_bot
    from dotenv import load_dotenv
except ImportError as e:
    print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
    sys.exit(1)

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

class DockerJobRunner:
    """Dockerå®¹å™¨ä¸­çš„èŒä½ç”³è¯·è¿è¡Œå™¨"""
    
    def __init__(self, container_id: str = None):
        self.container_id = container_id or os.getenv("CONTAINER_ID", "unknown")
        self.results_dir = "/app/results"
        self.ensure_results_dir()
    
    def ensure_results_dir(self):
        """ç¡®ä¿ç»“æœç›®å½•å­˜åœ¨"""
        os.makedirs(self.results_dir, exist_ok=True)
    
    async def run_single_job(self, job_data: Dict, backend: str = "browser-use") -> Dict:
        """è¿è¡Œå•ä¸ªèŒä½ç”³è¯·"""
        print(f"ğŸ³ å®¹å™¨ {self.container_id} å¼€å§‹å¤„ç†èŒä½: {job_data.get('title', 'Unknown')}")
        
        try:
            # åˆ›å»ºæœºå™¨äººå®ä¾‹
            bot = await create_unified_bot(
                backend=backend,
                config_path="/app/config/browser_config.json",
                job_data=job_data
            )
            
            # æ‰§è¡Œç”³è¯·
            result = await bot.apply_to_job()
            
            # æ¸…ç†èµ„æº
            await bot.cleanup()
            
            # è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
            result_dict = {
                "container_id": self.container_id,
                "job_url": result.job_url,
                "job_title": result.job_title,
                "company": result.company,
                "status": result.status,
                "backend_used": result.backend_used,
                "error_message": result.error_message,
                "timestamp": result.timestamp,
                "processing_time": None  # å¯ä»¥æ·»åŠ å¤„ç†æ—¶é—´
            }
            
            print(f"âœ… å®¹å™¨ {self.container_id} å®ŒæˆèŒä½ç”³è¯·: {result.status}")
            return result_dict
            
        except Exception as e:
            error_result = {
                "container_id": self.container_id,
                "job_url": job_data.get("url", ""),
                "job_title": job_data.get("title", "Unknown"),
                "company": job_data.get("company", "Unknown"),
                "status": "failed",
                "backend_used": backend,
                "error_message": str(e),
                "timestamp": datetime.now().isoformat(),
                "processing_time": None
            }
            
            print(f"âŒ å®¹å™¨ {self.container_id} ç”³è¯·å¤±è´¥: {str(e)}")
            return error_result
    
    async def run_job_batch(self, jobs: List[Dict], backend: str = "browser-use") -> List[Dict]:
        """è¿è¡Œä¸€æ‰¹èŒä½ç”³è¯·"""
        print(f"ğŸš€ å®¹å™¨ {self.container_id} å¼€å§‹å¤„ç† {len(jobs)} ä¸ªèŒä½")
        
        results = []
        for i, job_data in enumerate(jobs, 1):
            print(f"\nğŸ“ å¤„ç†ç¬¬ {i}/{len(jobs)} ä¸ªèŒä½")
            
            result = await self.run_single_job(job_data, backend)
            results.append(result)
            
            # ä¿å­˜ä¸­é—´ç»“æœ
            await self.save_intermediate_result(result)
            
            # æ·»åŠ å»¶è¿Ÿé¿å…è¿‡äºé¢‘ç¹çš„è¯·æ±‚
            if i < len(jobs):
                await asyncio.sleep(2)
        
        # ä¿å­˜æœ€ç»ˆç»“æœ
        await self.save_batch_results(results)
        
        print(f"ğŸ‰ å®¹å™¨ {self.container_id} å®Œæˆæ‰¹å¤„ç†")
        return results
    
    async def save_intermediate_result(self, result: Dict):
        """ä¿å­˜ä¸­é—´ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"result_{self.container_id}_{timestamp}.json"
            filepath = os.path.join(self.results_dir, filename)
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(result, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"âš ï¸ ä¿å­˜ä¸­é—´ç»“æœå¤±è´¥: {str(e)}")
    
    async def save_batch_results(self, results: List[Dict]):
        """ä¿å­˜æ‰¹æ¬¡ç»“æœ"""
        try:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"batch_{self.container_id}_{timestamp}.json"
            filepath = os.path.join(self.results_dir, filename)
            
            batch_summary = {
                "container_id": self.container_id,
                "timestamp": timestamp,
                "total_jobs": len(results),
                "successful": len([r for r in results if r["status"] == "success"]),
                "failed": len([r for r in results if r["status"] == "failed"]),
                "results": results
            }
            
            with open(filepath, 'w', encoding='utf-8') as f:
                json.dump(batch_summary, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ æ‰¹æ¬¡ç»“æœå·²ä¿å­˜: {filepath}")
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ‰¹æ¬¡ç»“æœå¤±è´¥: {str(e)}")

def parse_arguments():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Dockerå®¹å™¨èŒä½ç”³è¯·è¿è¡Œå™¨")
    
    parser.add_argument(
        "--jobs",
        type=str,
        help="èŒä½æ•°æ®JSONå­—ç¬¦ä¸²æˆ–æ–‡ä»¶è·¯å¾„"
    )
    
    parser.add_argument(
        "--backend",
        type=str,
        default="browser-use",
        choices=["browser-use", "openai-computer-use"],
        help="é€‰æ‹©è‡ªåŠ¨åŒ–åç«¯"
    )
    
    parser.add_argument(
        "--container-id",
        type=str,
        help="å®¹å™¨IDæ ‡è¯†"
    )
    
    return parser.parse_args()

async def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ³ Dockerå®¹å™¨èŒä½ç”³è¯·è¿è¡Œå™¨å¯åŠ¨")
    
    # è§£æå‚æ•°
    args = parse_arguments()
    
    # æ£€æŸ¥å¿…è¦çš„ç¯å¢ƒå˜é‡
    if not os.getenv("OPENAI_API_KEY"):
        print("âŒ ç¼ºå°‘ OPENAI_API_KEY ç¯å¢ƒå˜é‡")
        sys.exit(1)
    
    # è·å–èŒä½æ•°æ®
    jobs_data = []
    
    if args.jobs:
        try:
            # å°è¯•ä½œä¸ºJSONå­—ç¬¦ä¸²è§£æ
            if args.jobs.startswith('[') or args.jobs.startswith('{'):
                jobs_data = json.loads(args.jobs)
            else:
                # å°è¯•ä½œä¸ºæ–‡ä»¶è·¯å¾„è¯»å–
                with open(args.jobs, 'r', encoding='utf-8') as f:
                    jobs_data = json.load(f)
        except Exception as e:
            print(f"âŒ è§£æèŒä½æ•°æ®å¤±è´¥: {str(e)}")
            sys.exit(1)
    else:

        print(f"âŒ è§£æç¯å¢ƒå˜é‡ä¸­çš„èŒä½æ•°æ®å¤±è´¥: {str(e)}")
        sys.exit(1)

    if not jobs_data:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°èŒä½æ•°æ®")
        sys.exit(1)
    
    # ç¡®ä¿jobs_dataæ˜¯åˆ—è¡¨
    if isinstance(jobs_data, dict):
        jobs_data = [jobs_data]
    
    print(f"ğŸ“‹ å‡†å¤‡å¤„ç† {len(jobs_data)} ä¸ªèŒä½")
    
    # åˆ›å»ºè¿è¡Œå™¨
    runner = DockerJobRunner(container_id=args.container_id)
    
    # è¿è¡Œæ‰¹å¤„ç†
    try:
        results = await runner.run_job_batch(jobs_data, args.backend)
        
        # è¾“å‡ºæ‘˜è¦
        successful = len([r for r in results if r["status"] == "success"])
        failed = len([r for r in results if r["status"] == "failed"])
        
        print(f"\nğŸ“Š å®¹å™¨ {runner.container_id} æ‰§è¡Œæ‘˜è¦:")
        print(f"   âœ… æˆåŠŸ: {successful}")
        print(f"   âŒ å¤±è´¥: {failed}")
        print(f"   ğŸ“ ç»“æœå·²ä¿å­˜åˆ°: {runner.results_dir}")
        
        # è¿”å›é€‚å½“çš„é€€å‡ºç 
        sys.exit(0 if failed == 0 else 1)
        
    except Exception as e:
        print(f"âŒ æ‰¹å¤„ç†æ‰§è¡Œå¤±è´¥: {str(e)}")
        sys.exit(1)

if __name__ == "__main__":
    # è¿è¡Œä¸»å‡½æ•°
    asyncio.run(main())
